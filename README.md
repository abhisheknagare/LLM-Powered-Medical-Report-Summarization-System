# ğŸ¥ LLM-Powered Medical Report Summarization System

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-yellow)](https://huggingface.co/)

A production-ready system that leverages Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) to automatically summarize medical reports and extract clinical findings from the MIMIC-IV dataset.

## ğŸ¯ Project Overview

This project fine-tunes **TinyLlama-1.1B** using **LoRA** (Low-Rank Adaptation) on clinical notes from the MIMIC-IV dataset to generate accurate medical report summaries. The system implements a **RAG pipeline** using LangChain and vector databases to achieve **82% accuracy** in extracting clinical findings.

## âœ¨ Key Features

- ğŸ¤– **Fine-tuned LLM**: TinyLlama-1.1B optimized for medical text using LoRA
- ğŸ” **RAG Pipeline**: Retrieval-Augmented Generation for context-aware summaries
- ğŸ“Š **Clinical Finding Extraction**: 82% accuracy in identifying key medical information
- ğŸ¥ **MIMIC-IV Integration**: Trained on real-world clinical notes from ICU patients
- âš¡ **Production-Ready**: Scalable architecture suitable for healthcare applications
- ğŸ” **Privacy-Compliant**: Built with HIPAA considerations in mind

## ğŸ—ï¸ System Architecture
```
Medical Report Input
        â†“
Text Preprocessing & Embedding
        â†“
Vector Database (Qdrant)
        â†“
RAG Pipeline (LangChain)
        â†“
Fine-tuned TinyLlama (LoRA)
        â†“
Summary + Clinical Findings Output
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Base Model** | TinyLlama-1.1B |
| **Fine-tuning** | LoRA (Parameter-Efficient Fine-Tuning) |
| **RAG Framework** | LangChain |
| **Vector Database** | Qdrant |
| **Embeddings** | sentence-transformers |
| **Dataset** | MIMIC-IV Clinical Notes |
| **Language** | Python 3.8+ |
| **Deep Learning** | PyTorch, Hugging Face Transformers |

## ğŸ“‹ Prerequisites
```bash
Python 3.8 or higher
CUDA-capable GPU (recommended for training)
8GB+ RAM
Access to MIMIC-IV dataset (requires PhysioNet credentialing)
```

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/abhisheknagare/LLM-Powered-Medical-Report-Summarization-System.git
cd LLM-Powered-Medical-Report-Summarization-System
```

### 2. Set Up Environment
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download MIMIC-IV Dataset
```bash
# 1. Complete PhysioNet credentialing: https://physionet.org/
# 2. Download MIMIC-IV dataset
# 3. Place in data/mimic-iv/ directory
```

## ğŸ’» Usage

### Training the Model
```python
# Fine-tune TinyLlama with LoRA on MIMIC-IV data
python train.py \
    --model_name TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
    --dataset_path data/mimic-iv/notes \
    --output_dir models/tinyllama-medical \
    --lora_r 8 \
    --lora_alpha 16 \
    --num_epochs 3 \
    --batch_size 4
```

### Inference Example
```python
from medical_summarizer import MedicalReportSummarizer

# Initialize the summarizer
summarizer = MedicalReportSummarizer(
    model_path="models/tinyllama-medical",
    vector_db_path="data/vector_db"
)

# Sample medical report
report = """
Chief Complaint: Chest pain
History of Present Illness: 65-year-old male presents with substernal chest pain...
Physical Examination: BP 140/90, HR 88, RR 16, Temp 98.6Â°F
Assessment and Plan: Rule out acute coronary syndrome...
"""

# Generate summary
summary = summarizer.summarize(report)
print("Summary:", summary)

# Extract clinical findings
findings = summarizer.extract_findings(report)
print("Clinical Findings:", findings)
```

### RAG Pipeline Usage
```python
from rag_pipeline import RAGPipeline

# Initialize RAG system
rag = RAGPipeline(
    llm_model="models/tinyllama-medical",
    embedding_model="sentence-transformers/all-MiniLM-L6-v2",
    vector_db_type="faiss"
)

# Query with context retrieval
query = "What are the patient's vital signs and chief complaint?"
response = rag.query(query, report_text)
print(response)
```

## ğŸ“Š Performance Metrics

| Metric | Score | Details |
|--------|-------|---------|
| Clinical Finding Extraction | **82%** | Accuracy on MIMIC-IV test set |
| Model Size | 1.1B parameters | Optimized with LoRA |
| Inference Speed | ~2-3 seconds | Per average report (GPU) |
| Fine-tuning Method | LoRA | Rank 8, Alpha 16 |

### Evaluation Results

- **Precision**: High accuracy in identifying diagnoses, medications, and vital signs
- **Recall**: Effectively captures key clinical information from unstructured text
- **F1-Score**: Balanced performance across different clinical finding categories

## ğŸ“ Project Structure
```
LLM-Powered-Medical-Report-Summarization-System/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mimic-iv/              # MIMIC-IV dataset (not included)
â”‚   â”œâ”€â”€ vector_db/             # Vector database storage
â”‚   â””â”€â”€ processed/             # Preprocessed data
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tinyllama-medical/     # Fine-tuned model checkpoints
â”‚   â””â”€â”€ embeddings/            # Embedding models
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb
â”‚   â””â”€â”€ 03_evaluation.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # Model training script
â”‚   â”œâ”€â”€ inference.py           # Inference utilities
â”‚   â”œâ”€â”€ medical_summarizer.py  # Main summarizer class
â”‚   â”œâ”€â”€ rag_pipeline.py        # RAG implementation
â”‚   â”œâ”€â”€ data_processing.py     # Data preprocessing
â”‚   â””â”€â”€ evaluation.py          # Evaluation metrics
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_summarizer.py
â”‚
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup.py                   # Package setup
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ”¬ Technical Details

### Model Architecture

**Base Model**: TinyLlama-1.1B-Chat-v1.0
- Decoder-only transformer architecture
- 1.1 billion parameters
- Optimized for efficient inference

**LoRA Configuration**:
```python
lora_config = {
    "r": 8,                    # Rank of update matrices
    "lora_alpha": 16,          # Scaling factor
    "target_modules": ["q_proj", "v_proj"],
    "lora_dropout": 0.05,
    "bias": "none"
}
```

### RAG Pipeline

**Components**:
1. **Document Chunking**: Splits reports into semantic chunks
2. **Embedding Generation**: Creates vector representations using sentence-transformers
3. **Vector Storage**: Qdrant for efficient similarity search
4. **Retrieval**: Top-k relevant chunks based on query
5. **Generation**: LLM generates response using retrieved context

**Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2`
- 384-dimensional embeddings
- Balanced performance and speed

## ğŸ” Data Privacy & Compliance

âš ï¸ **IMPORTANT NOTICE**

This project uses the **MIMIC-IV dataset**, which contains de-identified patient data:

- âœ… All data is de-identified per HIPAA Safe Harbor standards
- âœ… PhysioNet credentialing required for dataset access
- âœ… No patient-identifiable information is processed or stored
- âš ï¸ For production use, ensure compliance with local regulations (HIPAA, GDPR, etc.)
- âš ï¸ Clinical validation required before deployment in healthcare settings

**PhysioNet Access**: https://physionet.org/content/mimiciv/

## ğŸ“ Key Learnings

This project demonstrates:
- **Parameter-efficient fine-tuning** using LoRA for domain adaptation
- **RAG architecture** for grounding LLM outputs in factual medical data
- **Vector databases** for efficient semantic search in clinical text
- **Clinical NLP** challenges and solutions
- **Production considerations** for healthcare AI systems

## ğŸ›£ï¸ Roadmap

- [x] Fine-tune TinyLlama on MIMIC-IV
- [x] Implement RAG pipeline
- [x] Achieve 82% extraction accuracy
- [ ] Add support for multiple medical report formats (HL7, FHIR)
- [ ] Implement real-time inference API (FastAPI)
- [ ] Create web interface for report upload
- [ ] Multi-language support (Spanish, French)
- [ ] Evaluation on additional clinical datasets
- [ ] Model quantization for edge deployment
- [ ] Integration with EHR systems

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add some AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

Please ensure:
- Code follows PEP 8 style guidelines
- Tests pass for new features
- Documentation is updated

## ğŸ› Known Issues

- Large reports (>2000 tokens) may require chunking for optimal performance
- GPU memory requirements: minimum 8GB VRAM for training
- RAG retrieval quality depends on embedding model choice

## ğŸ™ Acknowledgments

- **MIMIC-IV Dataset**: Johnson, A., Bulgarelli, L., Pollard, T., et al. (MIT LCP)
- **TinyLlama Team**: For the efficient base model
- **Hugging Face**: Transformers library and PEFT implementation
- **LangChain**: RAG framework and utilities
- **PhysioNet**: For providing access to clinical datasets

## ğŸ“š References

1. Johnson, A. E. W. et al. (2023). MIMIC-IV (version 2.2). PhysioNet.
2. Hu, E. J. et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ICLR 2022.
3. Lewis, P. et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020.

## ğŸ“§ Contact

**Abhishek Nagare**

- ğŸ“§ Email: abhisheknagare01@gmail.com
- ğŸ’¼ LinkedIn: [linkedin.com/in/abhishekmnagare](https://www.linkedin.com/in/abhishekmnagare)
- ğŸ™ GitHub: [@abhisheknagare](https://github.com/abhisheknagare)

## ğŸ“– Citation

If you use this project in your research or work, please cite:
```bibtex
@software{nagare2025medical_llm,
  author = {Nagare, Abhishek},
  title = {LLM-Powered Medical Report Summarization System},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/abhisheknagare/LLM-Powered-Medical-Report-Summarization-System}
}
```

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=abhisheknagare/LLM-Powered-Medical-Report-Summarization-System&type=Date)](https://star-history.com/#abhisheknagare/LLM-Powered-Medical-Report-Summarization-System&Date)

---

<div align="center">

**Made with â¤ï¸ for Healthcare AI**

If you find this project useful, please consider giving it a â­!

</div>

---

> **Disclaimer**: This is an academic/research project. Not intended for clinical use without proper validation, regulatory approval, and clinical oversight. Always consult healthcare professionals for medical decisions.
